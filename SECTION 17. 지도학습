SECTION 17. 지도학습
· 예측기법 : 회귀분석, 선형모형, 비선형모형
· 분류기법 : 의사결정나무, 서포트 벡터 머신, 판별분석, 로지스틱 회귀분석
· 학습 데이터는 분류 알고리즘을 사용해서 분류모형을 만들고, 검증 데이터로 분류모형의 타당성을 확인 해야한다.

· 다중회귀분석 변수 선택 : 전진선택법 / 후진제거법 / 단계별방법
	- 단계별방법 : 전진 선택법에 의해서 변수를 추가한다. 변수가 추가되고 중요도가
		      유의 수준에 포함되지 않으면 추가한 변수가 제거된다.
· K-인접기법 : 모델을 생성하지 않고 데이터를 분류하거나 예측할 때 사용하는 방법
	      비선형 모델 / Instance-based Learning / 유클리드 거리값 사용

· 의사결정나무 : 의사결정규칙 분류 및 예측
	- 분류나무(CART, C4.5, CHAID) + 회귀나무(CART)
	- 장점 : if~then으로 규칙 생성이 가능하고 이해하기 쉽다
		설명력이 좋기 때문에 많이 사용된다.
	- 분석절차 : 의사결정나무 형성 - 가지치기 - 최적 Tree 분류 - 해석 및 예측
	- 불순도 알고리즘 : 지니지수, 엔트로피지수, 카이제곱 통계량 등
		1. CART
		- 지니지수 : 범주형 / 불확실성을 의미하여 0에 가까울수록 좋은 것
			    즉, 0이라는 것은 100%를 모두 맞춘 것, 순수도가 가장 높음
		- 이진분리 : 연속형인 경우 분산을 사용
		- 분류나무와 회귀나무에 모두 사용 가능
	
		2. C4.5
		- 엔트로피지수 사용

		3. CHAID
		- 카이제곱 검정, F검정을 사용해서 다지분리하고, 
		  변수들 간의 관계는 의사결정트리를 통해서 표현된다.

		4. 회귀나무
		- 목표변수의 평균에 기초하여 트리를 형성한다.
		- CART는 평균을 이용하여 집단의 평균을 가지고 구분하기 때문에 
		  Leaf noad(단말노드)는 평균값을 가진다.

· 랜덤포레스트 : 분석과정에서 다수의 결정트리로부터 분류하거나 평균 예측치를 분석
		앙상블 기법 사용 / 여러 개의 의사결정나무를 만들고 투표를 통해서 다수결로 결과를 결정 / 배깅 기법 중 하나 / 부트스트랩 기법을 사용하여 여러 개의 샘플 데이터를 추출
	- 앙상블 : 여러 개의 분류 모델을 활용하여 최적의 모델을 선택하거나 여러개의 변수를 입력하여 어떤 변수가 가장 좋은 변수인지를 확인한다.
		- 투표 / 배깅 / 부스팅
	- 배깅 : 하나의 모델을 사용하지만 훈련세트를 여러 개의 샘플로 만들고 알고리즘마다 다른 훈련세트를 사용하는 방법
		- 부트스트랩 -> 데이터 추출 시에 중복을 허용하기 때문에 훈련세트에서 다양한 데이터가 추출된다.
	- 부스팅 : AdaBoose는 전체 훈련 세트를 사용하고 잘못 분류된 데이터에 가중치를 적용한다.

· 서포트 벡터 머신 : 초평면을 찾아서 이를 이용하여 분류 및 회귀분석을 수행
		   데이터를 분류할 수 있는 선
	- 선형 SVM : 초평면을 기준으로 객체 간에 마진을 최대화 되도록 분류하는 모델

· 나이브 베이즈 분류 : 데이터를 분류할 때 데이터 셋의 모든 특징들이 독립적이고 동등하다고 가정하고 데이터를 분류
	- 장점 : 매우 단순하고 결측 데이터가 있어도 우수 / 적은 학습데이터 가능 / 메모리 사용량이 적음 / 우수한 분류 성능 / 계산 복잡성 낮아 성능 빠름 / 예측에 대한 추정된 확률을 얻기 쉬움
	- 단점 : 모든 속성은 독립적이고 동등하다는 가정에 의존 / 독립되지 않은 경우에 오류 발생 가능 / 수치속성 데이터 셋에는 우수하지 않음 / 추정된 확률은 예측된 범주보다 신뢰가 떨어진다.

· 주성분 분석 : 데이터의 포인트를 가장 잘 구별해주는 변수인 주성분을 찾는 방법
		요약된 변수는 모든 X변수의 선형조합으로 생성 / 분산을 최대한 보존하는 축을 찾고, 그 축에 데이터를 사영
		고차원 데이터 분석에서 변수를 선택하거나 변수를 축소하는 분석방법

· 신경망 : 데이터 예측의 주요 개념으로 학습 기능을 모방하는 방법, 데이터를 분류하거나 예측할 때 모두 사용 / 결합함수는 선형함수이다.
	- 퍼셉트론 : 다수의 입력값과 가중치를 선형으로 결합, 활성화 함수에 따라서 생성되는 출력값이 결정
	- 활성화함수 : 어떤 신호를 입력 받아서 적절한 처리를 하고 출력하는 함수
		      1이면 다른 퍼셉트론에게 그 결과값을 전달하고 0이면 전달하지 않는다.
		- Sigmoid function : 항상 0과 1사이의 값만 가질 수 있는 비선형 함수
		- ReLU function : sigmoid function의 gradient vanishing 문제를 해결하기 위해서 사용 / 딥러닝에서 많이 사용
	- back-propagation : 비용값이 0이 되도록 가중치를 조정하여 다시 입력값으로 전송 / 어느 시점까지 계속 탐색하게 하는 모멘텀 설정
		- 모멘텀 : 어느정도 기존의 방향을 유지할 것인지를 조정
	- 신경망의 문제점 : 신경망의 포화상태와 과대적합(포화는 신경망에서 학습하면 할수록 활성화 함수에서 만들어질 수 없는 더 큰 결과값을 만들기 위해서 더 큰 가중치를 시도하게 되므로 신경망이 포화가 되게 된다.)
